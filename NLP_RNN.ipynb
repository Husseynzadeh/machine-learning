{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Sentiment Analysis"
      ],
      "metadata": {
        "id": "AFAaJJafZV_G"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R0_QVlgCJxX_"
      },
      "outputs": [],
      "source": [
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "\n",
        "raw_train_set, raw_valid_set, raw_test_set = tfds.load(\n",
        "    name = 'imdb_reviews',\n",
        "    split = ['train[:90%]', 'train[90%:]', 'test'],\n",
        "    as_supervised= True\n",
        ")\n",
        "\n",
        "tf.random.set_seed(42)\n",
        "train_set = raw_train_set.shuffle(5000, seed=42).batch(32).prefetch(1)\n",
        "valid_set = raw_valid_set.batch(32).prefetch(1)\n",
        "test_set = raw_test_set.batch(32).prefetch(1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for review, label in raw_train_set.take(6):\n",
        "  print(review.numpy().decode('utf-8')[:250], '...')\n",
        "  print('Label:', label.numpy())"
      ],
      "metadata": {
        "id": "a9oQlUaSK7Zz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 1000\n",
        "text_vec_layer = tf.keras.layers.TextVectorization(max_tokens=vocab_size,\n",
        "                                                   standardize='lower_and_strip_punctuation')\n",
        "text_vec_layer.adapt(train_set.map(lambda review, label:review))"
      ],
      "metadata": {
        "id": "p4DwoxF-L4zz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_vec_layer.get_vocabulary()[:50]"
      ],
      "metadata": {
        "id": "zZei0Q0MNYtK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list(map(str, text_vec_layer.get_vocabulary()[-50:]))"
      ],
      "metadata": {
        "id": "M9qa3qwROCj5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_vec_layer(['i like it'])"
      ],
      "metadata": {
        "id": "PMq7hIXwPjh8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embed_layer = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=64)\n",
        "embed_layer(text_vec_layer(['it was a great movie']))"
      ],
      "metadata": {
        "id": "qJv9hQ4CO3me"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embed_size = 128\n",
        "model = tf.keras.Sequential([\n",
        "    text_vec_layer,\n",
        "    tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embed_size, mask_zero=True),\n",
        "    tf.keras.layers.GRU(128),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "\n",
        "])"
      ],
      "metadata": {
        "id": "AFpV-biQbxc5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer=tf.keras.optimizers.Nadam(learning_rate=1e-3),\n",
        "              metrics = ['accuracy'])\n",
        "model.fit(train_set, validation_data=valid_set, epochs=2)"
      ],
      "metadata": {
        "id": "S37lcMGtc95M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_weights = model.layers[1].get_weights()[0]"
      ],
      "metadata": {
        "id": "_G13q5theydX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "np.savetxt('embedding.tsv', embedding_weights, delimiter='\\t')"
      ],
      "metadata": {
        "id": "Yj24mcaLfdCW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = text_vec_layer.get_vocabulary()\n",
        "\n",
        "with open('metadata.tsv', 'w', encoding='utf-8') as f:\n",
        "  for word in vocab:\n",
        "    word = word if word.strip() != '' else '<PAD>'\n",
        "    f.write(f'{word}\\n')"
      ],
      "metadata": {
        "id": "eY3zM2NUfquT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generating Shakespearean Text Using a Character RNN"
      ],
      "metadata": {
        "id": "lSYQPyS6gQaV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "shakespeare_url =  \"https://homl.info/shakespeare\"\n",
        "filepath = tf.keras.utils.get_file('shakespeare.txt', shakespeare_url)\n",
        "with open(filepath) as f:\n",
        "  shakespeare_text = f.read()"
      ],
      "metadata": {
        "id": "KJwUjme9jL0_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(shakespeare_text[:100])"
      ],
      "metadata": {
        "id": "X0UZaMn0jzBh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''.join(sorted(set(shakespeare_text.lower())))"
      ],
      "metadata": {
        "id": "nZShWAwVkh5P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_vec_layer = tf.keras.layers.TextVectorization(split = 'character', standardize = 'lower')\n",
        "text_vec_layer.adapt(shakespeare_text)\n",
        "encoded = text_vec_layer([shakespeare_text][0])"
      ],
      "metadata": {
        "id": "r43tgIQVkqQJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded"
      ],
      "metadata": {
        "id": "lfxoW_X2auIp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded -= 2\n",
        "n_tokens = text_vec_layer.vocabulary_size()-2\n",
        "dataset_size = len(encoded)"
      ],
      "metadata": {
        "id": "I8uCnYnvlOyS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_tokens"
      ],
      "metadata": {
        "id": "Vv7pubf8lQ5U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_size"
      ],
      "metadata": {
        "id": "WMKRH7V7l-eg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#input -> Before we proceed any furthe\n",
        "#output -> efore we proceed any further"
      ],
      "metadata": {
        "id": "8Dq7-uL7mAbS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def to_dataset(sequence , lenght ,seed =None,shuffle = False,batch_size = 32):\n",
        "  ds = tf.data.Dataset.from_tensor_slices(sequence)\n",
        "  ds = ds.window(lenght+1,shift=1,drop_remainder = True)\n",
        "  ds = ds.flat_map(lambda window_ds: window_ds.batch(lenght+1))\n",
        "  if shuffle :\n",
        "    ds = ds.shuffle(100_000,seed=seed)\n",
        "  ds = ds.batch(batch_size)\n",
        "  return ds.map(lambda window:(window[:,:-1],window[:,1:]))"
      ],
      "metadata": {
        "id": "lqlHq5JddwPi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lenght = 100\n",
        "tf.random.set_seed(42)\n",
        "train_set = to_dataset(encoded[:1_000_000],lenght=lenght,shuffle=True,\n",
        "                       seed=42)\n",
        "valid_set = to_dataset(encoded[1_000_000:1_060_000],lenght=lenght)\n",
        "test_set = to_dataset(encoded[1_060_000:],lenght=lenght)"
      ],
      "metadata": {
        "id": "XH9AzmujdyZo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(input_dim = vocab_size,output_dim=16),\n",
        "    tf.keras.layers.GRU(128,return_sequences = True),\n",
        "    tf.keras.layers.Dense(vocab_size,activation=\"softmax\")\n",
        "])\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\",optimizer=\"nadam\",\n",
        "              metrics = [\"accuracy\"])\n",
        "\n",
        "model_ckpt = tf.keras.callbacks.ModelCheckpoint(\n",
        "    \"my_shakespeare_model.keras\",\n",
        "    monitor=\"val_accuracy\",\n",
        "    save_best_only = True)\n",
        "\n",
        "history = model.fit(train_set,validation_data=valid_set,epochs=1,\n",
        "                    callbacks=[model_ckpt])"
      ],
      "metadata": {
        "id": "VCfHXE-Gd0vb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shakespeare_model = tf.keras.Sequential([\n",
        "    text_vec_layer,\n",
        "    tf.keras.layers.Lambda(lambda X:X-2),\n",
        "    model\n",
        "])"
      ],
      "metadata": {
        "id": "2cndE2-3d6Cb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_proba = shakespeare_model.predict(tf.constant([\"To be or not to b\"]))[0,-1]\n",
        "y_pred = tf.argmax(y_proba)\n",
        "text_vec_layer.get_vocabulary()[y_pred+2]"
      ],
      "metadata": {
        "id": "78pElCivd8Ys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QauT60V2s6-w"
      },
      "source": [
        "#Generating Fake Shakespeare text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k4FSm_7xuI8l"
      },
      "outputs": [],
      "source": [
        "log_probas = tf.math.log([[0.5,0.3,0.2]])\n",
        "tf.random.categorical(log_probas,num_samples=8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BF04GxeAuctx"
      },
      "outputs": [],
      "source": [
        "def next_char(text,temperature):\n",
        "  text = tf.constant([text])\n",
        "  y_proba = shakespeare_model.predict(text)[0,-1:]\n",
        "  rescaled_logits = tf.math.log(y_proba)/temperature\n",
        "  char_id = tf.random.categorical(rescaled_logits,num_samples=1)[0,0]\n",
        "  return text_vec_layer.get_vocabulary()[char_id+2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "er2bJTTbvspH"
      },
      "outputs": [],
      "source": [
        "def extent_text(text,chars=50,temperature = 1):\n",
        "  for _ in range(chars):\n",
        "    text +=next_char(text,temperature)\n",
        "  return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AV7OO7pxxR-x"
      },
      "outputs": [],
      "source": [
        "extent_text('to be or not to be',chars = 100,temperature = 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5688cIxxZOs"
      },
      "source": [
        "#An Encoder-Decoder Network for Neural Machine Translation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b1ux-Ckx0Ehh"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import tensorflow as tf\n",
        "\n",
        "url = \"https://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\"\n",
        "path = tf.keras.utils.get_file(\"spa-eng.zip\", origin=url, cache_dir=\"datasets\", extract=True)\n",
        "\n",
        "# Final corrected path\n",
        "spa_txt_path = Path(path).parent / \"spa-eng_extracted\" / \"spa-eng\" / \"spa.txt\"\n",
        "\n",
        "# Read the file\n",
        "text = spa_txt_path.read_text(encoding='utf-8')\n",
        "print(text[:500])  # Print first 500 characters as a quick check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B27mlr7F0JHp"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "text = text.replace(\"i\",\"\").replace(\"¿\",\"\")\n",
        "pairs = [line.split('\\t')for line in text.splitlines()]\n",
        "np.random.seed(42)\n",
        "np.random.shuffle(pairs)\n",
        "sentences_en ,sentences_es = zip(*pairs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sEDSIxs405u0"
      },
      "outputs": [],
      "source": [
        "pairs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zeVsZOej1cwm"
      },
      "outputs": [],
      "source": [
        "for i in range(3):\n",
        "  print(sentences_en[i],\"=>\",sentences_es[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fbOp7MP32Bsn"
      },
      "outputs": [],
      "source": [
        "vocab_size = 10000\n",
        "max_lenght = 50\n",
        "\n",
        "text_vec_layer_en = tf.keras.layers.TextVectorization(vocab_size,\n",
        "                                                      output_sequence_length = max_lenght)\n",
        "\n",
        "text_vec_layer_es = tf.keras.layers.TextVectorization(vocab_size,\n",
        "                                                      output_sequence_length = max_lenght)\n",
        "\n",
        "text_vec_layer_en.adapt(sentences_en)\n",
        "text_vec_layer_es.adapt([f'startofseq {s} endofseq' for s in sentences_es])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_vec_layer_en.get_vocabulary()[:10]"
      ],
      "metadata": {
        "id": "4aXCur7x7VAs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_vec_layer_es.get_vocabulary()[:10]"
      ],
      "metadata": {
        "id": "Nhcvrkxk8Hqn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = tf.constant(sentences_en[:100_000])\n",
        "X_valid = tf.constant(sentences_es[100_000:])\n",
        "X_train_dec = tf.constant([f'startofseq {s}' for s in sentences_es[:100_000]])\n",
        "X_valid_dec = tf.constant([f'startofseq {s}' for s in sentences_es[100_000:]])\n",
        "Y_train = text_vec_layer_es([f'{s} endofseq' for s in sentences_es[:100_000]])\n",
        "Y_valid = text_vec_layer_es([f'{s} endofseq' for s in sentences_es[100_000:]])"
      ],
      "metadata": {
        "id": "cOIkC4nA8K_u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(42)\n",
        "encoder_inputs = tf.keras.layers.Input(shape=[], dtype=tf.string)\n",
        "decoder_inputs = tf.keras.layers.Input(shape=[], dtype=tf.string)"
      ],
      "metadata": {
        "id": "A96lSal59moh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embed_size = 128\n",
        "encoder_input_ids = text_vec_layer_en(encoder_inputs)\n",
        "decoder_input_ids = text_vec_layer_es(decoder_inputs)\n",
        "encoder_embedding_layer = tf.keras.layers.Embedding(vocab_size, embed_size,\n",
        "                                                    mask_zero=True)\n",
        "decoder_embedding_layer = tf.keras.layers.Embedding(vocab_size, embed_size, mask_zero=True)\n",
        "\n",
        "encoder_embeddings = encoder_embedding_layer(encoder_input_ids)\n",
        "decoder_embeddings = decoder_embedding_layer(decoder_input_ids)\n"
      ],
      "metadata": {
        "id": "R-Qz14Ue-y-S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = tf.keras.layers.LSTM(512, return_state=True)    #son vectoru qaytarsin deye state\n",
        "encoder_outputs, *encoder_state = encoder(encoder_embeddings)\n",
        "\n",
        "\n",
        "decoder = tf.keras.layers.LSTM(512, return_sequences=True)   #multioutput kimi her defe output versin deye sequence\n",
        "decoder_outputs = decoder(decoder_embeddings, initial_state=encoder_state)"
      ],
      "metadata": {
        "id": "CMeOQ4Uh_tQ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_layer = tf.keras.layers.Dense(vocab_size, activation='softmax')\n",
        "Y_proba = output_layer(decoder_outputs)"
      ],
      "metadata": {
        "id": "VwJrKtPxArZ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Model(inputs = [encoder_inputs, decoder_inputs],\n",
        "                       outputs = [Y_proba])\n",
        "\n",
        "model.compile(loss = 'sparse_categorical_crossentropy',\n",
        "              optimizer = 'nadam',\n",
        "              metrics = ['accuracy'])\n",
        "\n",
        "# model.fit((X_train, X_train_dec), Y_train, epochs=3,\n",
        "#           validation_data=[(X_valid, X_valid_dec), Y_valid])"
      ],
      "metadata": {
        "id": "MCdKKnGQBbQL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def translate(sentence_en):\n",
        "  translation = ''\n",
        "  for word_idx in range(max_lenght):\n",
        "    X = tf.constant([sentence_en])\n",
        "    X_dec = tf.constant(['startofseq' + translation])\n",
        "    y_proba = model.predict((X,X_dec))[0, word_idx]\n",
        "    predicted_word_id = np.argmax(y_proba)\n",
        "    predicted_word = text_vec_layer_es.get_vocabulary()[predicted_word_id]\n",
        "    if predicted_word == 'endofseq':\n",
        "      break\n",
        "    translation += ' ' + predicted_word\n",
        "  return translation.strip()"
      ],
      "metadata": {
        "id": "myrvmVxMCJ8g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# translate('i love soccer')"
      ],
      "metadata": {
        "id": "BUcXA8RpJ0J4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bidirectional RNN"
      ],
      "metadata": {
        "id": "XbyaCr3SJ6xX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(42)\n",
        "encoder = tf.keras.layers.Bidirectional(\n",
        "    tf.keras.layers.LSTM(256, return_state = True)\n",
        ")"
      ],
      "metadata": {
        "id": "XY-6E-Q7Nfeq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ConcatenateStates(tf.keras.layers.Layer):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "  def call(self, encoder_state):\n",
        "    return [tf.concat(encoder_state[::2], axis=-1),  #short_term(2,4,6)\n",
        "              tf.concat(encoder_state[1::2],axis=-1)]   #long_term(1,3,5)\n",
        "\n",
        "encoder_outputs, *encoder_state = encoder(encoder_embeddings)\n",
        "concat_states = ConcatenateStates()\n",
        "encoder_state = concat_states(encoder_state)"
      ],
      "metadata": {
        "id": "lV4SkD4uN1ko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoder = tf.keras.layers.LSTM(512, return_sequences = True)\n",
        "decoder_outputs = decoder(decoder_embeddings, initial_state=encoder_state)\n",
        "\n",
        "output_layer = tf.keras.layers.Dense(vocab_size, activation='softmax')\n",
        "Y_proba = output_layer(decoder_outputs)\n",
        "\n",
        "model = tf.keras.Model(inputs = [encoder_inputs, decoder_inputs], outputs=[Y_proba])\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy',\n",
        "              optimizer='nadam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# model.fit((X_train, X_train_dec), Y_train, epochs=3,\n",
        "#           validation_data = [(X_valid, X_valid_dec), Y_valid])"
      ],
      "metadata": {
        "id": "9jikZKr_Q2fL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# translate('I like soccer and also going to the beach')"
      ],
      "metadata": {
        "id": "4Anay2bYSZqF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Beam Search"
      ],
      "metadata": {
        "id": "Gd9OBY6HTIIC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def beam_search(sentence_en, beam_width, verbose=False):\n",
        "    X = tf.constant([sentence_en])  # encoder input\n",
        "    X_dec = tf.constant([\"startofseq\"])  # decoder input\n",
        "    y_proba = model.predict((X, X_dec))[0, 0]  # first token's probas\n",
        "    top_k = tf.math.top_k(y_proba, k=beam_width)\n",
        "    top_translations = [  # list of best (log_proba, translation)\n",
        "        (np.log(word_proba), text_vec_layer_es.get_vocabulary()[word_id])\n",
        "        for word_proba, word_id in zip(top_k.values, top_k.indices)\n",
        "    ]\n",
        "\n",
        "    # extra code – displays the top first words in verbose mode\n",
        "    if verbose:\n",
        "        print(\"Top first words:\", top_translations)\n",
        "\n",
        "    for idx in range(1, max_lenght):\n",
        "        candidates = []\n",
        "        for log_proba, translation in top_translations:\n",
        "            if translation.endswith(\"endofseq\"):\n",
        "                candidates.append((log_proba, translation))\n",
        "                continue  # translation is finished, so don't try to extend it\n",
        "            X = tf.constant([sentence_en])  # encoder input\n",
        "            X_dec = tf.constant([\"startofseq \" + translation])  # decoder input\n",
        "            y_proba = model.predict((X, X_dec))[0, idx]  # last token's proba\n",
        "            for word_id, word_proba in enumerate(y_proba):\n",
        "                word = text_vec_layer_es.get_vocabulary()[word_id]\n",
        "                candidates.append((log_proba + np.log(word_proba),\n",
        "                                   f\"{translation} {word}\"))\n",
        "        top_translations = sorted(candidates, reverse=True)[:beam_width]\n",
        "\n",
        "        # extra code – displays the top translation so far in verbose mode\n",
        "        if verbose:\n",
        "            print(\"Top translations so far:\", top_translations)\n",
        "\n",
        "        if all([tr.endswith(\"endofseq\") for _, tr in top_translations]):\n",
        "            return top_translations[0][1].replace(\"endofseq\", \"\").strip()"
      ],
      "metadata": {
        "id": "qkeTh-g0TkaO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_en = 'I love cats and dogs'\n",
        "translate(sentence_en)"
      ],
      "metadata": {
        "id": "sDtus4yZT8V9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# beam_search(sentence_en, beam_width=3, verbose=True)"
      ],
      "metadata": {
        "id": "M9GrHOxpUGsT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Attention Mechanisms"
      ],
      "metadata": {
        "id": "2szJxIBbUS6y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(42)\n",
        "encoder_inputs = tf.keras.layers.Input(shape=[], dtype=tf.string)\n",
        "decoder_inputs = tf.keras.layers.Input(shape=[], dtype=tf.string)"
      ],
      "metadata": {
        "id": "haz4fv3kV3Yh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embed_size =128\n",
        "\n",
        "encoder_inputs_idx = text_vec_layer_en(encoder_inputs)\n",
        "decoder_inputs_idx = text_vec_layer_es(decoder_inputs)\n",
        "\n",
        "encoder_embeddings_layer = tf.keras.layers.Embedding(vocab_size, output_dim=embed_size)\n",
        "decoder_embeddings_layer = tf.keras.layers.Embedding(vocab_size, output_dim=embed_size)\n",
        "\n",
        "encoder_embeddings = encoder_embeddings_layer(encoder_inputs_idx)\n",
        "decoder_embeddings = decoder_embeddings_layer(decoder_inputs_idx)\n",
        "# maskla attention error verir mask hissesini silirik"
      ],
      "metadata": {
        "id": "gEAuuwrgWK-K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = tf.keras.layers.Bidirectional(\n",
        "    tf.keras.layers.LSTM(256, return_sequences=True, return_state=True)\n",
        ")\n",
        "\n",
        "encoder_outputs, *encoder_state = encoder(encoder_embeddings)\n",
        "encoder_state = concat_states(encoder_state)"
      ],
      "metadata": {
        "id": "V3zv-9eHXCFO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoder = tf.keras.layers.LSTM(512, return_sequences=True)\n",
        "decoder_outputs = decoder(decoder_embeddings, initial_state=encoder_state)"
      ],
      "metadata": {
        "id": "Z2B6LWMGXj-e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attention_layer = tf.keras.layers.Attention()\n",
        "attention_outputs = attention_layer([decoder_outputs, encoder_outputs])\n",
        "output_layer = tf.keras.layers.Dense(vocab_size, activation='softmax')\n",
        "Y_proba = output_layer(attention_outputs)"
      ],
      "metadata": {
        "id": "nbsrX8rvYLPE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Model(inputs = [encoder_inputs, decoder_inputs], outputs = [Y_proba])\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy',\n",
        "              optimizer='nadam',\n",
        "              metrics = ['accuracy'])\n",
        "\n",
        "model.fit((X_train, X_train_dec), Y_train, epochs=3, validation_data=[(X_valid, X_valid_dec), Y_valid])"
      ],
      "metadata": {
        "id": "2NHI9JzwYuAA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translate('I like soccer and also going to the beach')"
      ],
      "metadata": {
        "id": "bvXxGSPlZzz1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer model"
      ],
      "metadata": {
        "id": "dK0LNspBdvxi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "vocab_size = 10000\n",
        "max_length = 50\n",
        "embed_size = 128\n",
        "num_heads = 5\n",
        "ff_dim = 512\n",
        "\n",
        "\n",
        "# input layers\n",
        "encoder_inputs = tf.keras.Input(shape=(None,), dtype=tf.int32, name='encoder_inputs')\n",
        "decoder_inputs = tf.keras.Input(shape=(None,), dtype=tf.int32, name='decoder_inputs')\n",
        "\n",
        "# embedding layer\n",
        "\n",
        "encoder_embedding_layer = tf.keras.layers.Embedding(vocab_size,\n",
        "                                                    output_dim=embed_size,\n",
        "                                                    mask_zero=True)\n",
        "decoder_embedding_layer = tf.keras.layers.Embedding(vocab_size,\n",
        "                                                    output_dim=embed_size,\n",
        "                                                    mask_zero=True)\n",
        "\n",
        "encoder_embeddings = encoder_embedding_layer(encoder_inputs)\n",
        "decoder_embeddings = decoder_embedding_layer(decoder_inputs)\n",
        "\n",
        "# positional embedding\n",
        "\n",
        "pos_embedding_layer = tf.keras.layers.Embedding(max_length, embed_size)\n",
        "positions_encoder = tf.keras.layers.Lambda(lambda x: tf.range(start=0, limit=tf.shape(x)[1],\n",
        "                                                              delta = 1))(encoder_inputs)\n",
        "positions_decoder = tf.keras.layers.Lambda(lambda x: tf.range(start=0, limit=tf.shape(x)[1],\n",
        "                                                              delta = 1))(decoder_inputs)\n",
        "\n",
        "pos_embed_enc = pos_embedding_layer(positions_encoder)\n",
        "pos_embed_dec = pos_embedding_layer(positions_decoder)\n",
        "\n",
        "# Add tokens and postional embeddings\n",
        "encoder_embed = encoder_embeddings + pos_embed_enc\n",
        "decoder_embed = decoder_embeddings + pos_embed_dec"
      ],
      "metadata": {
        "id": "dyKPXUAsj99j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Encoder self Attention\n",
        "encoder_attention = tf.keras.layers.MultiHeadAttention(num_heads=num_heads,\n",
        "                                                       key_dim = embed_size)(encoder_embed, encoder_embed)  #query, key encoder_embed olur\n",
        "encoder_attention = tf.keras.layers.LayerNormalization(epsilon = 1e-6)(encoder_embed + encoder_attention)\n",
        "\n",
        "\n",
        "# Encoder Feed Forward\n",
        "encoder_ff = tf.keras.layers.Dense(ff_dim, activation='relu')(encoder_attention)\n",
        "encoder_ff = tf.keras.layers.Dense(embed_size)(encoder_ff)\n",
        "encoder_outputs = tf.keras.layers.LayerNormalization(epsilon=(1e-6))(encoder_attention + encoder_ff)"
      ],
      "metadata": {
        "id": "2JnuuLFmmUbr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Decoder self attention\n",
        "casual_mask = tf.keras.layers.Lambda(lambda x: tf.linalg.band_part(tf.ones((tf.shape(x)[1], tf.shape(x)[1])), -1, 0))(decoder_inputs)\n",
        "decoder_attention = tf.keras.layers.MultiHeadAttention(num_heads=num_heads,\n",
        "                                                       key_dim = embed_size)(decoder_embed, decoder_embed, attention_mask = casual_mask)\n",
        "decoder_attention = tf.keras.layers.LayerNormalization(epsilon=1e-6)(decoder_embed+decoder_attention)"
      ],
      "metadata": {
        "id": "dymEbs1NpaoE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Decoder Encoder cross attetion\n",
        "cross_attention = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_size)(decoder_attention,\n",
        "                                                                                              encoder_outputs, encoder_outputs)\n",
        "decoder_cross = tf.keras.layers.LayerNormalization(epsilon=1e-6)(decoder_attention+cross_attention)"
      ],
      "metadata": {
        "id": "UNYUcFOttWRq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Decoder Feed forward\n",
        "decoder_ff = tf.keras.layers.Dense(ff_dim, activation='relu')(decoder_cross)\n",
        "decoder_ff = tf.keras.layers.Dense(embed_size)(decoder_ff)\n",
        "decoder_outputs = tf.keras.layers.LayerNormalization(epsilon=1e-6)(decoder_cross+decoder_ff)"
      ],
      "metadata": {
        "id": "c4uRjjf6uNUw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Finel Output Layer\n",
        "output_logits = tf.keras.layers.Dense(vocab_size, activation='softmax')(decoder_outputs)\n",
        "transformer = tf.keras.Model([encoder_inputs, decoder_inputs], output_logits)"
      ],
      "metadata": {
        "id": "wXDo9zX_uuwY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformer.compile(loss='sparse_categorical_crossentropy',\n",
        "                    optimizer='nadam',\n",
        "                    metrics =['accuracy'])\n",
        "\n",
        "# transformer.fit((X_train, X_train_dec), Y_train, epochs=3, validation_data = ((X_valid, X_valid_dec), Y_valid))"
      ],
      "metadata": {
        "id": "jclm9YOgvWy4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 15000\n",
        "max_lenght = 50\n",
        "\n",
        "text_vec_layer_en = tf.keras.layers.TextVectorization(vocab_size,\n",
        "                                                      output_sequence_length = max_lenght,\n",
        "                                                      pad_to_max_tokens=True)\n",
        "\n",
        "text_vec_layer_es = tf.keras.layers.TextVectorization(vocab_size,\n",
        "                                                      output_sequence_length = max_lenght,\n",
        "                                                      pad_to_max_tokens = True)\n",
        "\n",
        "text_vec_layer_en.adapt(sentences_en)\n",
        "text_vec_layer_es.adapt([f'startofseq {s} endofseq' for s in sentences_es])"
      ],
      "metadata": {
        "id": "oYFjwF2-v4P0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_padded = tf.keras.preprocessing.sequence.pad_sequences(text_vec_layer_en(X_train).numpy(),\n",
        "                                                               padding='post', maxlen=max_lenght)\n",
        "X_train_dec_padded =tf.keras.preprocessing.sequence.pad_sequences(text_vec_layer_es(X_train_dec).numpy(),\n",
        "                                                                  padding='post', maxlen=max_lenght)\n",
        "X_valid_padded = tf.keras.preprocessing.sequence.pad_sequences(text_vec_layer_en(X_valid).numpy(),\n",
        "                                                               padding='post', maxlen=max_lenght)\n",
        "X_valid_dec_padded = tf.keras.preprocessing.sequence.pad_sequences(text_vec_layer_es(X_valid_dec).numpy(),\n",
        "                                                                   padding='post', maxlen=max_lenght)\n",
        "\n",
        "X_train_padded = tf.constant(X_train_padded)\n",
        "X_train_dec_padded = tf.constant(X_train_dec_padded)\n",
        "X_valid_padded = tf.constant(X_valid_padded)\n",
        "X_valid_dec_padden = tf.constant(X_valid_dec_padded)\n",
        "\n",
        "transformer.fit((X_train_padded, X_train_dec_padded), Y_train, epochs=3,\n",
        "                validation_data = ((X_valid_padded, X_valid_dec_padded), Y_valid))"
      ],
      "metadata": {
        "id": "4mkbBwxExCAl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def translate(sentence_en):\n",
        "    # Tokenize and pad encoder input\n",
        "    X = text_vec_layer_en(tf.constant([sentence_en]))\n",
        "    X = tf.keras.preprocessing.sequence.pad_sequences(X.numpy(), padding=\"post\", maxlen=max_length)\n",
        "\n",
        "    # Start token\n",
        "    start_token = text_vec_layer_es(['startofseq'])[0][0]\n",
        "    end_token = text_vec_layer_es(['endofseq'])[0][0]\n",
        "\n",
        "    # Decoder input initialized with just the start token\n",
        "    decoder_input = [start_token]\n",
        "\n",
        "    for _ in range(max_length):\n",
        "        decoder_input_padded = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "            [decoder_input], maxlen=max_length, padding=\"post\"\n",
        "        )\n",
        "\n",
        "        y_proba = transformer.predict((X, decoder_input_padded), verbose=0)[0, len(decoder_input)-1]\n",
        "        predicted_word_id = np.argmax(y_proba)\n",
        "\n",
        "        if predicted_word_id == end_token:\n",
        "            break\n",
        "\n",
        "        decoder_input.append(predicted_word_id)\n",
        "\n",
        "    # Map tokens back to words\n",
        "    vocab = text_vec_layer_es.get_vocabulary()\n",
        "    translated_words = [vocab[token] for token in decoder_input[1:]]  # skip start token\n",
        "\n",
        "    return ' '.join(translated_words)"
      ],
      "metadata": {
        "id": "iNz0qGVaymTC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(translate('i like soccer and also going to the beach'))"
      ],
      "metadata": {
        "id": "JzYrjcAMz7Ap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IBWRHdK90E36"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}